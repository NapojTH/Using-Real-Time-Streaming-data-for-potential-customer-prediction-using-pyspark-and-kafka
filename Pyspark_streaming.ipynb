{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have create our producer, we will use it to create a spark streaming to use the information from our producer to get the prediction of Top-up as the following:\n",
    "\n",
    "\n",
    "**Note: all the code is derieved from the tutorial week 10-week11 with some modification**\n",
    "\n",
    "Firstly, we will connect our notebook to kafka and import the library. Then we will make a spark config as below:\n",
    "\n",
    "### Task 2.1 Import library and Make a Spark configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the library\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col, decode, expr,udf\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "#Making a spark config with a time set to UTC and local to 2\n",
    "conf = SparkConf().setAppName(\"Top-up data streaming\").setMaster(\"local[2]\")\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf = conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'UTC') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Ingest the streaming data into spark streaming\n",
    "\n",
    "Then, we will make a spark streaming version of our data we have receive from our producers based on two topics as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe streaming of customer topic\n",
    "customer_stream = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "  .option(\"subscribe\", \"Customer\") \\\n",
    "  .load()\n",
    "\n",
    "#Dataframe streaming of bureau topic\n",
    "bureau_stream = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "  .option(\"subscribe\", \"Bureau\")\\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Transform our spark streaming into a proper format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have load our streaming received from our producer to streaming dataframe, we will make it into a proper format that match our metadata below:\n",
    "\n",
    "First, we will select only the key and value of customer and bureau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the key and value of both customer and bureau streaming dataframe\n",
    "customer_df = customer_stream.selectExpr(\"CAST(key AS STRING) AS key_customer\", \"CAST(value AS STRING) AS value_customer\")\n",
    "bureau_df = bureau_stream.selectExpr(\"CAST(key AS STRING) AS key_bureau\", \"CAST(value AS STRING) AS value_bureau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the streaming data we received is turn into string format before sending, we will use `ArrayType(StructType` with list of `StructField` to make all the variable as a string type first before we change them later on as below for both customer and bureau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the Schema based on the metadata of customer data\n",
    "Customer_schema = ArrayType(StructType([StructField('ID', StringType(), True), \n",
    "    StructField('Frequency', StringType(), True),\n",
    "    StructField('InstlmentMode', StringType(), True),\n",
    "    StructField('LoanStatus', StringType(), True),\n",
    "    StructField('PaymentMode', StringType(), True),\n",
    "    StructField('BranchID', StringType(), True),\n",
    "    StructField('Area', StringType(), True),\n",
    "    StructField('Tenure', StringType(), True), #Int\n",
    "    StructField('AssetCost', StringType(), True), #Int\n",
    "    StructField('AmountFinance', StringType(), True), #INT\n",
    "    StructField('DisbursalAmount', StringType(), True), #INT\n",
    "    StructField('EMI', StringType(), True), #INT\n",
    "    StructField('DisbursalDate', StringType(), True),\n",
    "    StructField('MaturityDAte', StringType(), True),\n",
    "    StructField('AuthDate', StringType(), True),\n",
    "    StructField('AssetID', StringType(), True),\n",
    "    StructField('ManufacturerID', StringType(), True),\n",
    "    StructField('SupplierID', StringType(), True),\n",
    "    StructField('LTV', StringType(), True), #INT\n",
    "    StructField('SEX', StringType(), True),\n",
    "    StructField('AGE', StringType(), True), #INT\n",
    "    StructField('MonthlyIncome', StringType(), True), #INT\n",
    "    StructField('City', StringType(), True),\n",
    "    StructField('State', StringType(), True),\n",
    "    StructField('ZiPCODE', StringType(), True),\n",
    "    StructField('Top-up Month', StringType(), True),\n",
    "    StructField('ts', TimestampType(), True)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we cast each variable into it true datatype, we need to unnest the data by using `select` with `from_json` then apply `.explode`. With those command, we will get the data to be unnested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unnest the column from json form to unnest version of variable\n",
    "customer_df=customer_df.select(F.from_json(F.col(\"value_customer\").cast(\"string\"), Customer_schema).alias('parsed_value'))\n",
    "customer_df = customer_df.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))\n",
    "customer_df = customer_df.select(F.col('unnested_value.ID').alias('ID'),\n",
    "                                F.col('unnested_value.Frequency').alias('Frequency'),\n",
    "                                F.col('unnested_value.InstlmentMode').alias('InstlmentMode'),\n",
    "                                F.col('unnested_value.LoanStatus').alias('LoanStatus'),\n",
    "                                F.col('unnested_value.PaymentMode').alias('PaymentMode'),\n",
    "                                F.col('unnested_value.BranchID').alias('BranchID'),\n",
    "                                F.col('unnested_value.Area').alias('Area'),\n",
    "                                F.col('unnested_value.Tenure').alias('Tenure'),\n",
    "                                F.col('unnested_value.AssetCost').alias('AssetCost'),\n",
    "                                F.col('unnested_value.AmountFinance').alias('AmountFinance'),\n",
    "                                F.col('unnested_value.DisbursalAmount').alias('DisbursalAmount'),\n",
    "                                F.col('unnested_value.EMI').alias('EMI'),\n",
    "                                F.col('unnested_value.DisbursalDate').alias('DisbursalDate'),\n",
    "                                F.col('unnested_value.MaturityDAte').alias('MaturityDAte'),\n",
    "                                F.col('unnested_value.AuthDate').alias('AuthDate'),\n",
    "                                F.col('unnested_value.AssetID').alias('AssetID'),\n",
    "                                F.col('unnested_value.ManufacturerID').alias('ManufacturerID'),\n",
    "                                F.col('unnested_value.SupplierID').alias('SupplierID'),\n",
    "                                F.col('unnested_value.LTV').alias('LTV'),\n",
    "                                F.col('unnested_value.SEX').alias('SEX'),\n",
    "                                F.col('unnested_value.AGE').alias('AGE'),\n",
    "                                F.col('unnested_value.MonthlyIncome').alias('MonthlyIncome'),\n",
    "                                F.col('unnested_value.City').alias('City'),\n",
    "                                F.col('unnested_value.State').alias('State'),\n",
    "                                F.col('unnested_value.ZiPCODE').alias('ZiPCODE'),\n",
    "                                F.col('unnested_value.Top-up Month').alias('Top-up Month'),\n",
    "                                F.col('unnested_value.ts').alias('ts'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we will change the data type based on our metadata. The process will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cast the column datatype based on the metadata information\n",
    "customer_df = customer_df.withColumn('AssetCost',col('AssetCost').cast('Double'))\\\n",
    "              .withColumn('AmountFinance',col('AmountFinance').cast('Double'))\\\n",
    "              .withColumn('DisbursalAmount',col('DisbursalAmount').cast('Double'))\\\n",
    "              .withColumn('EMI',col('EMI').cast('Double'))\\\n",
    "              .withColumn('LTV',col('LTV').cast('Float'))\\\n",
    "              .withColumn('AGE',col('AGE').cast('Integer'))\\\n",
    "              .withColumn('MonthlyIncome',col('MonthlyIncome').cast('Float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the Schema \n",
    "bureau_schema = ArrayType(StructType([StructField('ID', StringType(), True), \n",
    "    StructField('SELF-INDICATOR', StringType(), True),\n",
    "    StructField('MATCH-TYPE', StringType(), True),\n",
    "    StructField('ACCT-TYPE', StringType(), True),\n",
    "    StructField('CONTRIBUTOR-TYPE', StringType(), True),\n",
    "    StructField('DATE-REPORTED', StringType(), True),\n",
    "    StructField('OWNERSHIP-IND', StringType(), True),\n",
    "    StructField('ACCOUNT-STATUS', StringType(), True),\n",
    "    StructField('DISBURSED-DT', StringType(), True),\n",
    "    StructField('CLOSE-DT', StringType(), True),\n",
    "    StructField('LAST-PAYMENT-DATE', StringType(), True), \n",
    "    StructField('CREDIT-LIMIT/SANC AMT', StringType(), True), #Int\n",
    "    StructField('DISBURSED-AMT/HIGH CREDIT', StringType(), True), #Int\n",
    "    StructField('INSTALLMENT-AMT', StringType(), True), #Int\n",
    "    StructField('CURRENT-BAL', StringType(), True), #Int\n",
    "    StructField('INSTALLMENT-FREQUENCY', StringType(), True),\n",
    "    StructField('OVERDUE-AMT', StringType(), True), #Int\n",
    "    StructField('WRITE-OFF-AMT', StringType(), True), #Int\n",
    "    StructField('ASSET_CLASS', StringType(), True),\n",
    "    StructField('REPORTED DATE - HIST', StringType(), True),\n",
    "    StructField('DPD - HIST', StringType(), True),\n",
    "    StructField('CUR BAL - HIST', StringType(), True),\n",
    "    StructField('AMT OVERDUE - HIST', StringType(), True),\n",
    "    StructField('AMT PAID - HIST', StringType(), True),\n",
    "    StructField('TENURE', StringType(), True), #int\n",
    "    StructField('ts', TimestampType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unnest the column from json form to unnest version of variable\n",
    "bureau_df=bureau_df.select(F.from_json(F.col(\"value_bureau\").cast(\"string\"), bureau_schema).alias('parsed_value'))\n",
    "bureau_df = bureau_df.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))\n",
    "bureau_df = bureau_df.select(F.col('unnested_value.ID').alias('ID'),\n",
    "                                F.col('unnested_value.SELF-INDICATOR').alias('SELF-INDICATOR'),\n",
    "                                F.col('unnested_value.MATCH-TYPE').alias('MATCH-TYPE'),\n",
    "                                F.col('unnested_value.ACCT-TYPE').alias('ACCT-TYPE'),\n",
    "                                F.col('unnested_value.CONTRIBUTOR-TYPE').alias('CONTRIBUTOR-TYPE'),\n",
    "                                F.col('unnested_value.DATE-REPORTED').alias('DATE-REPORTED'),\n",
    "                                F.col('unnested_value.OWNERSHIP-IND').alias('OWNERSHIP-IND'),\n",
    "                                F.col('unnested_value.ACCOUNT-STATUS').alias('ACCOUNT-STATUS'),\n",
    "                                F.col('unnested_value.DISBURSED-DT').alias('DISBURSED-DT'),\n",
    "                                F.col('unnested_value.CLOSE-DT').alias('CLOSE-DT'),\n",
    "                                F.col('unnested_value.LAST-PAYMENT-DATE').alias('LAST-PAYMENT-DATE'),\n",
    "                                F.col('unnested_value.CREDIT-LIMIT/SANC AMT').alias('CREDIT-LIMIT/SANC AMT'),\n",
    "                                F.col('unnested_value.DISBURSED-AMT/HIGH CREDIT').alias('DISBURSED-AMT/HIGH CREDIT'),\n",
    "                                F.col('unnested_value.INSTALLMENT-AMT').alias('INSTALLMENT-AMT'),\n",
    "                                F.col('unnested_value.CURRENT-BAL').alias('CURRENT-BAL'),\n",
    "                                F.col('unnested_value.INSTALLMENT-FREQUENCY').alias('INSTALLMENT-FREQUENCY'),\n",
    "                                F.col('unnested_value.OVERDUE-AMT').alias('OVERDUE-AMT'),\n",
    "                                F.col('unnested_value.WRITE-OFF-AMT').alias('WRITE-OFF-AMT'),\n",
    "                                F.col('unnested_value.ASSET_CLASS').alias('ASSET_CLASS'),\n",
    "                                F.col('unnested_value.REPORTED DATE - HIST').alias('REPORTED DATE - HIST'),\n",
    "                                F.col('unnested_value.DPD - HIST').alias('DPD - HIST'),\n",
    "                                F.col('unnested_value.CUR BAL - HIST').alias('CUR BAL - HIST'),\n",
    "                                F.col('unnested_value.AMT OVERDUE - HIST').alias('AMT OVERDUE - HIST'),\n",
    "                                F.col('unnested_value.AMT PAID - HIST').alias('AMT PAID - HIST'),\n",
    "                                F.col('unnested_value.TENURE').alias('TENURE'),\n",
    "                                F.col('unnested_value.ts').alias('ts'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference that occur for bureau dataframe is that some information might need to be manipulate in order to make their form correct. We can do this by remove some comma or postfix from these variable value as the follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format change column\n",
    "format_change = ['CREDIT-LIMIT/SANC AMT', 'DISBURSED-AMT/HIGH CREDIT', 'INSTALLMENT-AMT', 'CURRENT-BAL', 'OVERDUE-AMT']\n",
    "for cols in format_change:\n",
    "    #Remove comma\n",
    "    bureau_df = bureau_df.withColumn(cols,F.udf(lambda x: x.replace(',','') if x is not None\n",
    "                                               else None)(F.col(cols)))\n",
    "                                        \n",
    "#Change the value of 'INSTALLMENT-AMT' column (NNK,2022c)\n",
    "bureau_df=bureau_df.withColumn('INSTALLMENT-AMT',F.regexp_replace(F.col('INSTALLMENT-AMT'),\"[\\$#,A-Za-z/]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cast the datatype to the correct one based on the metadata\n",
    "bureau_df = bureau_df.withColumn('CREDIT-LIMIT/SANC AMT',col('CREDIT-LIMIT/SANC AMT').cast('Double'))\\\n",
    "              .withColumn('DISBURSED-AMT/HIGH CREDIT',col('DISBURSED-AMT/HIGH CREDIT').cast('Double'))\\\n",
    "              .withColumn('INSTALLMENT-AMT',col('INSTALLMENT-AMT').cast('Double'))\\\n",
    "              .withColumn('CURRENT-BAL',col('CURRENT-BAL').cast('Double'))\\\n",
    "              .withColumn('OVERDUE-AMT',col('OVERDUE-AMT').cast('Double'))\\\n",
    "              .withColumn('WRITE-OFF-AMT',col('WRITE-OFF-AMT').cast('Double'))\\\n",
    "              .withColumn('TENURE',col('TENURE').cast('Integer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we finish formating our dataframe now we will apply the watermark to them by using `.withWatermark` to both dataframe and setting them by using the `ts` variable with 5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the watermark to both bureau and customer dataframe\n",
    "customer_df = customer_df.withWatermark(\"ts\", \"5 seconds\")\n",
    "bureau_df = bureau_df.withWatermark(\"ts\", \"5 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Group the bureau stream based on ID with 30 seconds window duration.\n",
    "\n",
    "In this section, ther are two parts as the following:\n",
    "1. Change the `self-indicator` format to 1 and 0 for True and False value by using udf function and change it data type to match the value\n",
    "2. Grouping the bureau data based on ID. Then, setting window duration to 30 seconds when joing the data. This can be done by  combining of `groupBy` and `window` function on `ts` with 30 seconds. After that we will use aggregation for each variable based on their data type similar to task 2A. below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the bureau dataframe with the same rule as the last\n",
    "def change_boolean(s):\n",
    "    check = 0\n",
    "    if s == 'True':\n",
    "        check = 1\n",
    "    else:\n",
    "        check = 0\n",
    "    return check\n",
    "\n",
    "#Apply the udf to self-indicator and change the datatype based on the value\n",
    "bool_udf = udf(change_boolean,IntegerType())\n",
    "\n",
    "bureau_df = bureau_df.withColumn(\"SELF-INDICATOR\",bool_udf(\"SELF-INDICATOR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify which column is numeric and string so that we can group it in the next part\n",
    "numeric_column = [field.name for field in bureau_df.schema.fields if str(field.dataType) != 'StringType' \n",
    "                 if str(field.dataType) != 'TimestampType']\n",
    "string_column = [col for col in bureau_df.columns if col not in numeric_column if col != 'ID' if col != 'ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group bureau dataframe if the numeric column we sum it if it string we count distinct\n",
    "bureau_new_df = bureau_df.groupBy(window(bureau_df.ts, \"30 seconds\"),\"ID\")\\\n",
    "        .agg(F.sum(numeric_column[0]).alias(numeric_column[0]+'_sum'),\n",
    "            F.sum(numeric_column[1]).alias(numeric_column[1]+'_sum'),\n",
    "            F.sum(numeric_column[2]).alias(numeric_column[2]+'_sum'),\n",
    "            F.sum(numeric_column[3]).alias(numeric_column[3]+'_sum'),\n",
    "            F.sum(numeric_column[4]).alias(numeric_column[4]+'_sum'),\n",
    "            F.sum(numeric_column[5]).alias(numeric_column[5]+'_sum'),\n",
    "            F.sum(numeric_column[6]).alias(numeric_column[6]+'_sum'),\n",
    "            F.sum(numeric_column[7]).alias(numeric_column[7]+'_sum'),\n",
    "            F.approx_count_distinct(string_column[0]).alias(string_column[0]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[1]).alias(string_column[1]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[2]).alias(string_column[2]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[3]).alias(string_column[3]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[4]).alias(string_column[4]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[5]).alias(string_column[5]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[6]).alias(string_column[6]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[7]).alias(string_column[7]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[8]).alias(string_column[8]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[9]).alias(string_column[9]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[10]).alias(string_column[10]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[11]).alias(string_column[11]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[12]).alias(string_column[12]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[13]).alias(string_column[13]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[14]).alias(string_column[14]+'_dist'),\n",
    "            F.approx_count_distinct(string_column[15]).alias(string_column[15]+'_dist'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Join the two streaming dataframes based on window duration\n",
    "\n",
    "After we have finish grouping, we have to create new variables called 'window_start' and 'window_end' based on the start and the end of window duration variable below before joing two dataframes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create window_start and window_end column\n",
    "bureau_new_df = bureau_new_df.withColumn('window_start',bureau_new_df.window.start)\\\n",
    "                .withColumn('window_end',bureau_new_df.window.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- SELF-INDICATOR_sum: long (nullable = true)\n",
      " |-- CREDIT-LIMIT/SANC AMT_sum: double (nullable = true)\n",
      " |-- DISBURSED-AMT/HIGH CREDIT_sum: double (nullable = true)\n",
      " |-- INSTALLMENT-AMT_sum: double (nullable = true)\n",
      " |-- CURRENT-BAL_sum: double (nullable = true)\n",
      " |-- OVERDUE-AMT_sum: double (nullable = true)\n",
      " |-- WRITE-OFF-AMT_sum: double (nullable = true)\n",
      " |-- TENURE_sum: long (nullable = true)\n",
      " |-- MATCH-TYPE_dist: long (nullable = false)\n",
      " |-- ACCT-TYPE_dist: long (nullable = false)\n",
      " |-- CONTRIBUTOR-TYPE_dist: long (nullable = false)\n",
      " |-- DATE-REPORTED_dist: long (nullable = false)\n",
      " |-- OWNERSHIP-IND_dist: long (nullable = false)\n",
      " |-- ACCOUNT-STATUS_dist: long (nullable = false)\n",
      " |-- DISBURSED-DT_dist: long (nullable = false)\n",
      " |-- CLOSE-DT_dist: long (nullable = false)\n",
      " |-- LAST-PAYMENT-DATE_dist: long (nullable = false)\n",
      " |-- INSTALLMENT-FREQUENCY_dist: long (nullable = false)\n",
      " |-- ASSET_CLASS_dist: long (nullable = false)\n",
      " |-- REPORTED DATE - HIST_dist: long (nullable = false)\n",
      " |-- DPD - HIST_dist: long (nullable = false)\n",
      " |-- CUR BAL - HIST_dist: long (nullable = false)\n",
      " |-- AMT OVERDUE - HIST_dist: long (nullable = false)\n",
      " |-- AMT PAID - HIST_dist: long (nullable = false)\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check whether we succesfully create the variable by using printSchema\n",
    "bureau_new_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we will change the name of the `ID` variable from both customer and bureau dataframes. Next, we will join them based on the `ID` and `window_start` and `window_end` if the `ts` of customer is between both window start and window end time and have the same ID a bureau then join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change customer and new bureau dataframe ID so it can be join\n",
    "customer_df = customer_df.withColumnRenamed('ID','customer_ID')\n",
    "bureau_new_df = bureau_new_df.withColumnRenamed('ID','bureau_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join two dataframe based on the described above condition\n",
    "merge_df = customer_df.join(bureau_new_df,expr(\"\"\"customer_ID = bureau_ID AND ts>=window_start AND ts<=window_end\"\"\"),\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename ID after joining\n",
    "merge_df = merge_df.withColumnRenamed('customer_ID','ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Frequency: string (nullable = true)\n",
      " |-- InstlmentMode: string (nullable = true)\n",
      " |-- LoanStatus: string (nullable = true)\n",
      " |-- PaymentMode: string (nullable = true)\n",
      " |-- BranchID: string (nullable = true)\n",
      " |-- Area: string (nullable = true)\n",
      " |-- Tenure: string (nullable = true)\n",
      " |-- AssetCost: double (nullable = true)\n",
      " |-- AmountFinance: double (nullable = true)\n",
      " |-- DisbursalAmount: double (nullable = true)\n",
      " |-- EMI: double (nullable = true)\n",
      " |-- DisbursalDate: string (nullable = true)\n",
      " |-- MaturityDAte: string (nullable = true)\n",
      " |-- AuthDate: string (nullable = true)\n",
      " |-- AssetID: string (nullable = true)\n",
      " |-- ManufacturerID: string (nullable = true)\n",
      " |-- SupplierID: string (nullable = true)\n",
      " |-- LTV: float (nullable = true)\n",
      " |-- SEX: string (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- MonthlyIncome: float (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- ZiPCODE: string (nullable = true)\n",
      " |-- Top-up Month: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- bureau_ID: string (nullable = true)\n",
      " |-- SELF-INDICATOR_sum: long (nullable = true)\n",
      " |-- CREDIT-LIMIT/SANC AMT_sum: double (nullable = true)\n",
      " |-- DISBURSED-AMT/HIGH CREDIT_sum: double (nullable = true)\n",
      " |-- INSTALLMENT-AMT_sum: double (nullable = true)\n",
      " |-- CURRENT-BAL_sum: double (nullable = true)\n",
      " |-- OVERDUE-AMT_sum: double (nullable = true)\n",
      " |-- WRITE-OFF-AMT_sum: double (nullable = true)\n",
      " |-- TENURE_sum: long (nullable = true)\n",
      " |-- MATCH-TYPE_dist: long (nullable = false)\n",
      " |-- ACCT-TYPE_dist: long (nullable = false)\n",
      " |-- CONTRIBUTOR-TYPE_dist: long (nullable = false)\n",
      " |-- DATE-REPORTED_dist: long (nullable = false)\n",
      " |-- OWNERSHIP-IND_dist: long (nullable = false)\n",
      " |-- ACCOUNT-STATUS_dist: long (nullable = false)\n",
      " |-- DISBURSED-DT_dist: long (nullable = false)\n",
      " |-- CLOSE-DT_dist: long (nullable = false)\n",
      " |-- LAST-PAYMENT-DATE_dist: long (nullable = false)\n",
      " |-- INSTALLMENT-FREQUENCY_dist: long (nullable = false)\n",
      " |-- ASSET_CLASS_dist: long (nullable = false)\n",
      " |-- REPORTED DATE - HIST_dist: long (nullable = false)\n",
      " |-- DPD - HIST_dist: long (nullable = false)\n",
      " |-- CUR BAL - HIST_dist: long (nullable = false)\n",
      " |-- AMT OVERDUE - HIST_dist: long (nullable = false)\n",
      " |-- AMT PAID - HIST_dist: long (nullable = false)\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merge_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Store our new join dataframe in a paquet format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have done joining two dataframes, we will select these columns: `ID`,`window_start`,`window_end`,`ts`,`Top-up Month`. Then we will rename the `Top-up month` to `Top-up_Month`before we parquet the file and store in some location as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the column based on instruction and rename the top-up month\n",
    "df_format1 = merge_df.select('ID','window_start','window_end','ts','Top-up Month')\n",
    "df_format1 = df_format1.withColumnRenamed('Top-up Month','Top-up_Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parquet form output for 2.5 (wait for it to save the data for 1-2 minutes)\n",
    "query_file_sink = df_format1.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/task2B_2.6\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/task2B_2.6/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop the storing \n",
    "query_file_sink.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we stop the query for creating parquet, we will show the result to make sure that we did not get an empty result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- Top-up_Month: string (nullable = true)\n",
      "\n",
      "+---+-------------------+-------------------+-------------------+-----------------+\n",
      "| ID|       window_start|         window_end|                 ts|     Top-up_Month|\n",
      "+---+-------------------+-------------------+-------------------+-----------------+\n",
      "|244|2022-10-18 02:17:00|2022-10-18 02:17:30|2022-10-18 02:17:10|No Top-up Service|\n",
      "|246|2022-10-18 02:17:00|2022-10-18 02:17:30|2022-10-18 02:17:10|No Top-up Service|\n",
      "|229|2022-10-18 02:17:00|2022-10-18 02:17:30|2022-10-18 02:17:10|No Top-up Service|\n",
      "|171|2022-10-18 02:16:30|2022-10-18 02:17:00|2022-10-18 02:16:54|No Top-up Service|\n",
      "|183|2022-10-18 02:16:30|2022-10-18 02:17:00|2022-10-18 02:16:54|No Top-up Service|\n",
      "|164|2022-10-18 02:16:30|2022-10-18 02:17:00|2022-10-18 02:16:45|No Top-up Service|\n",
      "|147|2022-10-18 02:16:30|2022-10-18 02:17:00|2022-10-18 02:16:45|No Top-up Service|\n",
      "|220|2022-10-18 02:17:00|2022-10-18 02:17:30|2022-10-18 02:17:02|No Top-up Service|\n",
      "|197|2022-10-18 02:17:00|2022-10-18 02:17:30|2022-10-18 02:17:02|No Top-up Service|\n",
      "|294|2022-10-18 02:17:00|2022-10-18 02:17:30|2022-10-18 02:17:29|No Top-up Service|\n",
      "|293|2022-10-18 02:17:00|2022-10-18 02:17:30|2022-10-18 02:17:29|No Top-up Service|\n",
      "|277|2022-10-18 02:17:00|2022-10-18 02:17:30|2022-10-18 02:17:19|No Top-up Service|\n",
      "|269|2022-10-18 02:17:00|2022-10-18 02:17:30|2022-10-18 02:17:19|No Top-up Service|\n",
      "|240|2022-10-18 02:17:00|2022-10-18 02:17:30|2022-10-18 02:17:10|No Top-up Service|\n",
      "|243|2022-10-18 02:17:00|2022-10-18 02:17:30|2022-10-18 02:17:10|No Top-up Service|\n",
      "|235|2022-10-18 02:17:00|2022-10-18 02:17:30|2022-10-18 02:17:10|No Top-up Service|\n",
      "|225|2022-10-18 02:17:00|2022-10-18 02:17:30|2022-10-18 02:17:10|No Top-up Service|\n",
      "| 90|2022-10-18 02:16:00|2022-10-18 02:16:30|2022-10-18 02:16:28|No Top-up Service|\n",
      "|104|2022-10-18 02:16:00|2022-10-18 02:16:30|2022-10-18 02:16:28|No Top-up Service|\n",
      "|101|2022-10-18 02:16:00|2022-10-18 02:16:30|2022-10-18 02:16:28|No Top-up Service|\n",
      "+---+-------------------+-------------------+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read the parquet file in the location and show it\n",
    "query_file_sink_df = spark.read.parquet(\"parquet/task2B_2.6\")\n",
    "query_file_sink_df.printSchema()\n",
    "query_file_sink_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we will remove the directory that store the parquet so that when we run the notebook again we will not get an error.\n",
    "\n",
    "**Note: This step will be repeat in the next two parts as well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the path for task 2.6 if people want a new rerun\n",
    "home_path = os.getcwd()\n",
    "\n",
    "import shutil\n",
    "def remove_folder(path):\n",
    "    # check if folder exists\n",
    "    if os.path.exists(path):\n",
    "         # remove if exists\n",
    "         shutil.rmtree(path)\n",
    "    \n",
    "remove_folder(home_path+\"/parquet/task2B_2.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7  Import machine learning models and make a prediction\n",
    "\n",
    "In the following, we will import our machine learning model using `PipelineModel` to load our model in the directory and we will `setHandleInvalid` to be keep in all of the steps to ignore the null on the dataframe. Once we have done that, we will transform our join dataframe using `.transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 2.7 making a prediction using our ML pipeline model\n",
    "model = PipelineModel.load('topup_pipeline_model')\n",
    "model.stages[-2].setHandleInvalid(\"keep\") \n",
    "model.stages[-3].setHandleInvalid(\"keep\")\n",
    "model.stages[-4].setHandleInvalid(\"keep\")\n",
    "model_pred = model.transform(merge_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have apply the transformation and getting a prediction from our machine learning model, will repeat the same steps as the previous section but this time we will put select the `prediction` variable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the top-up month name and select the variable for showing\n",
    "model_pred = model_pred.withColumnRenamed('Top-up Month','Top-up_Month')\n",
    "df_pred =  model_pred.select('ID','window_start','window_end','ts','prediction','Top-up_Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parquet form output for 2.7 (wait for the  2 minutes to 3 minutes to see the result in the next code chunk)\n",
    "query_file_sink2 = df_pred\\\n",
    "        .writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/task2B_2.7\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/task2B_2.7/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop the query\n",
    "query_file_sink2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- Top-up_Month: string (nullable = true)\n",
      "\n",
      "+---+-------------------+-------------------+-------------------+----------+-----------------+\n",
      "| ID|       window_start|         window_end|                 ts|prediction|     Top-up_Month|\n",
      "+---+-------------------+-------------------+-------------------+----------+-----------------+\n",
      "|771|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:58|       0.0|No Top-up Service|\n",
      "|760|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:58|       0.0|No Top-up Service|\n",
      "|707|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:41|       1.0|No Top-up Service|\n",
      "|717|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:41|       1.0|No Top-up Service|\n",
      "|740|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:49|       0.0|No Top-up Service|\n",
      "|747|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:49|       1.0|No Top-up Service|\n",
      "|703|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:41|       0.0|No Top-up Service|\n",
      "|724|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:41|       1.0|No Top-up Service|\n",
      "|768|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:58|       0.0|No Top-up Service|\n",
      "|763|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:58|       1.0|No Top-up Service|\n",
      "|700|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:32|       0.0|No Top-up Service|\n",
      "|686|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:32|       0.0|No Top-up Service|\n",
      "|710|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:41|       0.0|No Top-up Service|\n",
      "|737|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:49|       0.0|No Top-up Service|\n",
      "|782|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:58|       0.0|No Top-up Service|\n",
      "|695|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:32|       0.0|No Top-up Service|\n",
      "|692|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:32|       0.0|No Top-up Service|\n",
      "|730|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:49|       1.0|No Top-up Service|\n",
      "|712|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:41|       0.0|No Top-up Service|\n",
      "|742|2022-10-18 02:19:30|2022-10-18 02:20:00|2022-10-18 02:19:49|       0.0|No Top-up Service|\n",
      "+---+-------------------+-------------------+-------------------+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show our result\n",
    "query_file_sink_df2 = spark.read.parquet(\"parquet/task2B_2.7\")\n",
    "query_file_sink_df2.printSchema()\n",
    "query_file_sink_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the path way for task 2.7\n",
    "remove_folder(home_path+\"/parquet/task2B_2.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Get count for top-up by each States "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last section for this task, it is beginning with filtering our dataframe that have a `prediction` result. The result we want is the `prediction` which is equal to `1` as those who will top-up. Then, we select two columns which is `window_end` and `State` and rename them as `key` and `value` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering that result of prediction to 1 and select our target variable.\n",
    "model_state = model_pred.filter(col('prediction')==1)\n",
    "model_state = model_state.withColumnRenamed('window_end','key')\n",
    "model_state = model_state.select('key','State')\n",
    "model_state = model_state.withColumnRenamed('State','value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get that, we will create a foreachbatch function which will handling the multiple aggreagation task and store our aggregated dataframe as a parquet to be used for sending to kafka consumer as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "# Task 2.8 Create function to show values received from input dataframe\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    \"\"\"\n",
    "    In this function, we only need dataframe a parameter and we will aggregate the dataframe\n",
    "    by changing it form by apply a groupby function and change the form of value to json format\n",
    "    consiste of States and count of each states as a value while the key is the window end time stamp\n",
    "    \"\"\"\n",
    "    print(f\"epoch_id: {epoch_id}; count:{df.count()}; now: {dt.datetime.now().strftime('%X')}\")\n",
    "    if df.count() > 0:\n",
    "        df = df.groupBy(\"key\",\"value\").agg(F.count('value').alias('count'))\n",
    "        df = df.withColumnRenamed('value','State')\n",
    "        df = df.select(\"key\",to_json(struct(\"State\",\"count\")).alias(\"json\")) #(wffzxyl,2019)\n",
    "        df = df.groupBy(\"key\").agg(F.collect_set(F.col(\"json\")).alias('value')) #(DBA108642,2020)\n",
    "        df.select(\"key\",\"value\").write.format(\"parquet\")\\\n",
    "        .mode('append')\\\n",
    "        .option(\"path\", \"parquet/task2B_2.8\")\\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we get our function, we will apply the `writestream` with our `foreachbatch` function to start store our result as a parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_id: 0; count:0; now: 14:18:40\n"
     ]
    }
   ],
   "source": [
    "#Wait until the count of dataframe have any number so you can put run the next chunk\n",
    "query1 = model_state.writeStream.outputMode(\"append\")\\\n",
    "        .foreachBatch(foreach_batch_function)\\\n",
    "        .trigger(processingTime='20 seconds')\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created a parquet, we will start transfer it to consumer by using a while loop of write and save to kafka with the read parquet we created from previous code. The loop need to be run and we need to interupt it to stop it. Lastly same as previous two sections, we will stop query and delete the directory so we can reuse this in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read our parquest created by the previous code\n",
    "final_query = spark.read.parquet(\"parquet/task2B_2.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_id: 1; count:0; now: 14:19:05\n",
      "epoch_id: 2; count:6; now: 14:19:34\n",
      "epoch_id: 3; count:22; now: 14:20:17\n",
      "epoch_id: 4; count:29; now: 14:20:55\n",
      "epoch_id: 5; count:29; now: 14:21:40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-a7b50fbcb4c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Run this to transfer our panquet to consumer side (don't run the next code chunk until we satisfied)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfinal_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CAST(key AS STRING)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"CAST(value AS STRING)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Run this to transfer our panquet to consumer side (don't run the next code chunk until we satisfied)\n",
    "while True:\n",
    "    final_query.selectExpr(\"CAST(key AS STRING)\",\"CAST(value AS STRING)\")\\\n",
    "        .write.mode(\"append\")\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "        .option(\"topic\", \"Top_up_count_state\") \\\n",
    "        .option(\"checkpointLocation\",home_path+\"/kafka/checkpoint\")\\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the directory for reuse the code in the future\n",
    "remove_folder(home_path+\"/parquet/task2B_2.8\")\n",
    "remove_folder(home_path+\"/kafka/checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DBA108642. (2020, Febuary 27). $\\textit{Pyspark merge multiple columns into a json column}$. https://stackoverflow.com/questions/60435907/pyspark-merge-multiple-columns-into-a-json-column?fbclid=IwAR2LQtmt97HHogT_d95vZzCm36TXo3XqJaYR7imaiswpJEbMT8OP9cSelLg\n",
    "- Jupyter Notebooks:FIT 5202 Data Processing in Big Data (2022). $\\textit{Week 10  Clickstream Spark Streaming - Handling Json Array DEMO}$.https://lms.monash.edu/mod/resource/view.php?id=10523282\n",
    "- Jupyter Notebooks:FIT 5202 Data Processing in Big Data (2022). $\\textit{Week 10 Lab-Task Log Analysis [V 1.1]}$.https://lms.monash.edu/mod/resource/view.php?id=10523278\n",
    "- Jupyter Notebooks:FIT 5202 Data Processing in Big Data (2022). $\\textit{Week 10  Spark Stream Join Example]}$.https://lms.monash.edu/mod/resource/view.php?id=10523281\n",
    "- Jupyter Notebooks:FIT 5202 Data Processing in Big Data (2022). $\\textit{Week 11  Spark Streaming Watermarking DEMO [V 1.1]}$. https://lms.monash.edu/mod/resource/view.php?id=10523304\n",
    "- NNK. (2022, August 5). $\\textit{PySpark Replace Column Values in DataFrame}$. https://sparkbyexamples.com/pyspark/pyspark-replace-column-values/\n",
    "- $\\textit{Structured Streaming Programming Guide}$.(nd.). https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "- $\\textit{Structured Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher)}$. (nd.). https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\n",
    "- wffzxyl. (2019, July 19). $\\textit{How to convert some pyspark dataframe's column into a dict with its column name and combine them to be a json column?}$. https://stackoverflow.com/questions/57112873/how-to-convert-some-pyspark-dataframes-column-into-a-dict-with-its-column-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
